{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from spn.structure.Base import Sum, Product, Context, get_nodes_by_type\n",
    "from spn.structure.leaves.parametric.Parametric import Gaussian, Categorical\n",
    "from spn.algorithms.LearningWrappers import learn_structure, learn_mspn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "from spn.algorithms.MPE import mpe\n",
    "from spn.algorithms.EM import EM_optimization\n",
    "from spn.structure.leaves.piecewise.PiecewiseLinear import create_histogram_leaf\n",
    "from spn.algorithms.splitting.RDC import get_split_cols_RDC_py\n",
    "from spn.algorithms.splitting.Clustering import get_split_rows_KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import random_region_graph, region_graph_to_spn, reassign_node_ids\n",
    "from datasets.utils import get_vertical_train_data, get_test_data\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from spn.algorithms.Gradient import gradient_backward\n",
    "from spn.algorithms.Inference import log_likelihood\n",
    "from spn.algorithms.Validity import is_valid\n",
    "\n",
    "from spn.structure.Base import Sum, get_nodes_by_type, get_number_of_nodes\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import timeit\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_scopes(spn, inds):\n",
    "    nodes = get_nodes_by_type(spn)\n",
    "    scope_mapping = {i: s for i, s in enumerate(inds)}\n",
    "    for n in nodes:\n",
    "        sc = list(n.scope)\n",
    "        new_sc = [scope_mapping[i] for i in sc]\n",
    "        n.scope = new_sc\n",
    "    return spn\n",
    "\n",
    "def reassign_ids(spn):\n",
    "    nodes = get_nodes_by_type(spn)\n",
    "    for i, n in enumerate(nodes):\n",
    "        n.id = i\n",
    "    return spn\n",
    "\n",
    "def make_dataset(num_samples, num_features, n_informative, n_redundant, n_classes, n_clusters, n_repeated):\n",
    "    x, y = make_classification(num_samples, num_features, n_informative=n_informative, \n",
    "                               n_redundant=n_redundant, n_classes=n_classes, n_clusters_per_class=n_clusters,\n",
    "                               n_repeated=n_repeated, class_sep=2)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3)\n",
    "    train_data = np.hstack([train_x, train_y.reshape(-1, 1)])\n",
    "    test_data = np.hstack([test_x, test_y.reshape(-1, 1)])\n",
    "    return train_data, test_data\n",
    "\n",
    "split_cols = get_split_cols_RDC_py(0.3, False)\n",
    "split_rows = get_split_rows_KMeans(2, standardize=False)\n",
    "\n",
    "def softmax(vec, temperature):\n",
    "    \"\"\"\n",
    "    turn vec into normalized probability\n",
    "    \"\"\"\n",
    "    sum_exp = sum(np.exp(x/temperature) for x in vec)\n",
    "    return np.array([np.exp(x/temperature)/sum_exp for x in vec])\n",
    "\n",
    "def cond_sum_em_update(allowed_nodes):\n",
    "    def sum_em_update(node, node_gradients=None, root_lls=None, all_lls=None, **kwargs):\n",
    "        if node.id in allowed_nodes:\n",
    "            RinvGrad = node_gradients - root_lls\n",
    "\n",
    "            for i, c in enumerate(node.children):\n",
    "                new_w = RinvGrad + (all_lls[:, c.id] + np.log(node.weights[i]))\n",
    "                node.weights[i] = logsumexp(new_w)\n",
    "\n",
    "            assert not np.any(np.isnan(node.weights))\n",
    "\n",
    "            node.weights = np.exp(node.weights - logsumexp(node.weights)) + np.exp(-100)\n",
    "\n",
    "            node.weights = node.weights / node.weights.sum()\n",
    "            #node.weights = softmax(node.weights, 0.1)\n",
    "            #idx = np.argsort(node.weights)[:-3]\n",
    "            #node.weights[idx] = 0\n",
    "            node.weights = node.weights / node.weights.sum()\n",
    "\n",
    "\n",
    "            if node.weights.sum() > 1:\n",
    "                node.weights[np.argmax(node.weights)] -= node.weights.sum() - 1\n",
    "\n",
    "            assert not np.any(np.isnan(node.weights))\n",
    "            assert np.isclose(np.sum(node.weights), 1)\n",
    "            assert not np.any(node.weights < 0)\n",
    "            assert node.weights.sum() <= 1, \"sum: {}, node weights: {}\".format(node.weights.sum(), node.weights)\n",
    "    return sum_em_update\n",
    "\n",
    "_node_updates = {Sum: cond_sum_em_update([0])}\n",
    "\n",
    "def add_node_em_update(node_type, lambda_func):\n",
    "    _node_updates[node_type] = lambda_func\n",
    "\n",
    "\n",
    "def EM_optimization_network(spn, data, iterations=5, node_updates=_node_updates, skip_validation=False, **kwargs):\n",
    "    if not skip_validation:\n",
    "        valid, err = is_valid(spn)\n",
    "        assert valid, \"invalid spn: \" + err\n",
    "\n",
    "    lls_per_node = np.zeros((data.shape[0], get_number_of_nodes(spn)))\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # one pass bottom up evaluating the likelihoods\n",
    "        log_likelihood(spn, data, dtype=data.dtype, lls_matrix=lls_per_node)\n",
    "\n",
    "        gradients = gradient_backward(spn, lls_per_node)\n",
    "\n",
    "        R = lls_per_node[:, 0]\n",
    "\n",
    "        for node_type, func in node_updates.items():\n",
    "            for node in get_nodes_by_type(spn, node_type):\n",
    "                func(\n",
    "                    node,\n",
    "                    node_lls=lls_per_node[:, node.id],\n",
    "                    node_gradients=gradients[:, node.id],\n",
    "                    root_lls=R,\n",
    "                    all_lls=lls_per_node,\n",
    "                    all_gradients=gradients,\n",
    "                    data=data,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "def build_fedspn_head(client_cluster_spns):\n",
    "    num_clients = len(client_cluster_spns)\n",
    "    # assume num clusters is equal on all clients \n",
    "    num_clusters = len(client_cluster_spns[0])\n",
    "    clusters = list(range(num_clusters))\n",
    "    prods = {}\n",
    "    for l in range(1, num_clients):\n",
    "        for comb in product(*[clusters]*num_clients):\n",
    "            prefix = list(comb)[:l]\n",
    "            next_node = list(comb)[l]\n",
    "            prod_id = tuple(prefix + [next_node])\n",
    "            if l > 1:\n",
    "                # connect product node of last layer with next_node's SPN of l-the client\n",
    "                relevant_spns = [prods[tuple(prefix)], client_cluster_spns[l][next_node]]\n",
    "            else:\n",
    "                # first product layer -> connect all client SPNs of a certain prod_id\n",
    "                relevant_spns = [client_cluster_spns[i][j] for i,j in enumerate(prod_id)]\n",
    "            scopes = [set(s.scope) for s in relevant_spns]\n",
    "            prod_scope = list(set().union(*scopes))\n",
    "            prod = Product(relevant_spns)\n",
    "            prod.scope = prod_scope\n",
    "            prods[prod_id] = prod\n",
    "\n",
    "    all_scopes = set()\n",
    "    for cluster_spns in client_cluster_spns:\n",
    "        for s in cluster_spns:\n",
    "            all_scopes = all_scopes.union(set(s.scope))\n",
    "    \n",
    "    root_children = [n for prefix, n in prods.items() if len(prefix) == num_clients]\n",
    "    weights = softmax(np.zeros(len(root_children)), 1)\n",
    "    #weights = softmax(np.random.normal(0, 0.5, len(root_children)))\n",
    "    root = Sum(weights, root_children)\n",
    "    root.scope = list(all_scopes)\n",
    "    root = reassign_node_ids(root)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20\n",
    "datasets = {}\n",
    "for num_corrs in [0, 10, 15]:\n",
    "    inf = num_features - num_corrs\n",
    "    train, test = make_dataset(2000, num_features, inf, num_corrs, 2, 1, 0)\n",
    "    datasets[num_corrs] = (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_node_type(data):\n",
    "    types = []\n",
    "    for i  in range(data.shape[1]):\n",
    "        unique = len(np.unique(data[:, i]))\n",
    "        if unique < 100:\n",
    "            params = {'p': np.repeat(1 / unique, unique)}\n",
    "            types.append((Categorical, params))\n",
    "        else:\n",
    "            params = {'mean': 0, 'stdev': 1}\n",
    "            types.append((Gaussian, params))\n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_fl_e2e(train_data, test_data, s, class_idx, spn_struct='rat'):\n",
    "    lls = []\n",
    "    runtimes = []\n",
    "    for _ in range(5):\n",
    "        # train one spn on each client\n",
    "        spns = []\n",
    "        client_rt = 0\n",
    "        for cl_idx in s:\n",
    "            start = timeit.default_timer()\n",
    "            client_data = train_data[:, cl_idx]\n",
    "            client_features = client_data.shape[1] - 1\n",
    "            kmeans = KMeans(2)\n",
    "            if class_idx in cl_idx:\n",
    "                context = Context(parametric_types=[Gaussian]*client_features + [Categorical]).add_domains(client_data)\n",
    "                clusters = kmeans.fit_predict(client_data)\n",
    "            else:\n",
    "                context = Context(parametric_types=[Gaussian]*(client_features + 1)).add_domains(client_data)\n",
    "                clusters = kmeans.fit_predict(client_data[:, :-1])\n",
    "            cluster_spns = []\n",
    "            for c in np.unique(clusters):\n",
    "                idx = np.argwhere(clusters == c).flatten()\n",
    "                subset = client_data[idx]\n",
    "                if spn_struct == 'learned':\n",
    "                    spn_classification = learn_structure(subset, context, split_rows, split_cols, create_histogram_leaf)\n",
    "                    spn_classification = map_scopes(spn_classification, cl_idx)\n",
    "                elif spn_struct == 'rat':\n",
    "                    rg = random_region_graph(0, list(range(client_features)), [])\n",
    "                    if class_idx in cl_idx:\n",
    "                        dists = {i: (Gaussian, {'mean': 0, 'stdev': 1}) for i in range(client_features)}\n",
    "                        dists[client_features] = (Categorical, {'p': [0.5, 0.5]})\n",
    "                    else:\n",
    "                        dists = {i: (Gaussian, {'mean': 0, 'stdev': 1}) for i in range(client_features+1)}\n",
    "                    curr_layer = [n for n in rg.nodes if len(list(rg.pred[n])) == 0]\n",
    "                    spn_classification = region_graph_to_spn(rg, curr_layer, dists)\n",
    "                    spn_classification = map_scopes(spn_classification, cl_idx)\n",
    "                    spn_classification = reassign_node_ids(spn_classification)\n",
    "                cluster_spns.append(spn_classification)\n",
    "            spns.append(cluster_spns)\n",
    "            t = timeit.default_timer() - start\n",
    "            client_rt = max(client_rt, t)\n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        spn = build_fedspn_head(spns)\n",
    "        # optimize server SPN\n",
    "        # NOTE: It's legal to put client data in here since we can propagate likelihoods\n",
    "        #   over the network without sending private information\n",
    "        EM_optimization(spn, train_data)\n",
    "        #print(spn.weights)\n",
    "        # evaluate server model\n",
    "        server_rt = timeit.default_timer() - start\n",
    "        overall_rt = client_rt + server_rt\n",
    "        runtimes.append(overall_rt)\n",
    "        ll = log_likelihood(spn, test_data)\n",
    "        lls.append(np.mean(ll))\n",
    "    return lls, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_fl_two_step(train_data, test_data, s, class_idx, spn_struct='rat'):\n",
    "    lls = []\n",
    "    runtimes = []\n",
    "    for _ in range(5):\n",
    "        # train one spn on each client\n",
    "        spns = []\n",
    "        client_rt = 0\n",
    "        for cl_idx in s:\n",
    "            start = timeit.default_timer()\n",
    "            client_data = train_data[:, cl_idx]\n",
    "            client_features = client_data.shape[1] - 1\n",
    "            kmeans = KMeans(2)\n",
    "            node_types = infer_node_type(client_data)\n",
    "            types = [t for t, _ in node_types]\n",
    "            context = Context(parametric_types=types).add_domains(client_data)\n",
    "            if class_idx in cl_idx:\n",
    "                clusters = kmeans.fit_predict(client_data)\n",
    "            else:\n",
    "                clusters = kmeans.fit_predict(client_data[:, :-1])\n",
    "            cluster_spns = []\n",
    "            for c in np.unique(clusters):\n",
    "                idx = np.argwhere(clusters == c).flatten()\n",
    "                subset = client_data[idx]\n",
    "                if spn_struct == 'learned':\n",
    "                    spn_classification = learn_structure(subset, context, split_rows, split_cols, create_histogram_leaf)\n",
    "                elif spn_struct == 'rat':\n",
    "                    rg = random_region_graph(0, list(range(client_features)), [])\n",
    "                    dists = {i: t for i, t in enumerate(node_types)}\n",
    "                    curr_layer = [n for n in rg.nodes if len(list(rg.pred[n])) == 0]\n",
    "                    spn_classification = region_graph_to_spn(rg, curr_layer, dists)\n",
    "                    spn_classification = reassign_node_ids(spn_classification)\n",
    "                EM_optimization(spn_classification, subset)\n",
    "                spn_classification = map_scopes(spn_classification, cl_idx)\n",
    "                cluster_spns.append(spn_classification)\n",
    "            spns.append(cluster_spns)\n",
    "            t = timeit.default_timer() - start\n",
    "            client_rt = max(client_rt, t)\n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        spn = build_fedspn_head(spns)\n",
    "        # optimize server SPN\n",
    "        # NOTE: It's legal to put client data in here since we can propagate likelihoods\n",
    "        #   over the network without sending private information\n",
    "        EM_optimization_network(spn, train_data)\n",
    "        #print(spn.weights)\n",
    "        server_rt = timeit.default_timer() - start\n",
    "        overall_rt = client_rt + server_rt\n",
    "        runtimes.append(overall_rt)\n",
    "        ll = log_likelihood(spn, test_data)\n",
    "        lls.append(np.mean(ll))\n",
    "    return lls, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_experiment(datasets, n_clients, training='e2e'):\n",
    "    log_likelihoods = []\n",
    "    for r in [0]:\n",
    "        print(f\"Train with r={r} redundant features\")\n",
    "        train_data, test_data = datasets[r]\n",
    "        # split data on n clients vertically\n",
    "        indices = np.arange(train_data.shape[1])\n",
    "        s = np.array_split(indices, n_clients)\n",
    "        class_idx = train_data.shape[1] - 1\n",
    "        if training == 'e2e':\n",
    "            lls, rt = vertical_fl_e2e(train_data, test_data, s, class_idx)\n",
    "        else:\n",
    "            lls, rt = vertical_fl_two_step(train_data, test_data, s, class_idx)\n",
    "        log_likelihoods.append(lls)\n",
    "    return np.array(log_likelihoods), rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_experiment(n_clients, training='e2e'):\n",
    "    log_likelihoods = []\n",
    "    train_data, _ = get_vertical_train_data('income', n_clients)\n",
    "    # due to compatibility first stack train_data columns\n",
    "    train_data = np.column_stack(train_data)\n",
    "    split_idx = int(0.7*len(train_data))\n",
    "    train_data, test_data = train_data[:split_idx], train_data[split_idx:]\n",
    "    # split data on n clients vertically\n",
    "    indices = np.arange(train_data.shape[1])\n",
    "    s = np.array_split(indices, n_clients)\n",
    "    class_idx = train_data.shape[1] - 1\n",
    "    if training == 'e2e':\n",
    "        lls, rt = vertical_fl_e2e(train_data, test_data, s, class_idx)\n",
    "    else:\n",
    "        lls, rt = vertical_fl_two_step(train_data, test_data, s, class_idx)\n",
    "    log_likelihoods.append(lls)\n",
    "    return np.array(log_likelihoods), rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_experiment(n_clients, training='e2e'):\n",
    "    train_data, _ = get_vertical_train_data('credit', n_clients)\n",
    "    test_data = get_test_data('credit')\n",
    "    # due to compatibility first stack train_data columns\n",
    "    train_data = np.column_stack(train_data)\n",
    "    # split data on n clients vertically\n",
    "    indices = np.arange(train_data.shape[1])\n",
    "    s = np.array_split(indices, n_clients)\n",
    "    class_idx = train_data.shape[1] - 1\n",
    "    if training == 'e2e':\n",
    "        lls, rt = vertical_fl_e2e(train_data, test_data, s, class_idx)\n",
    "    else:\n",
    "        lls, rt = vertical_fl_two_step(train_data, test_data, s, class_idx)\n",
    "    return lls, rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-17.62337467, -17.62337467, -17.62337467, -17.62337467,\n",
       "         -17.62337467]]),\n",
       " [15.583048567990772,\n",
       "  15.518587407976156,\n",
       "  15.651644947007298,\n",
       "  15.533854084991617,\n",
       "  15.590369198005646])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_experiment(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-34.56542603, -34.5749303 , -34.63581021, -34.58977622,\n",
       "         -34.57579776]]),\n",
       " [15.226520216994686,\n",
       "  15.522485764988232,\n",
       "  15.174898539989954,\n",
       "  15.342063400981715,\n",
       "  15.12149933699402])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_experiment(2, '2step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with r=0 redundant features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-46.92523213, -46.92523213, -46.92523213, -46.92523213,\n",
       "         -46.92523213]]),\n",
       " [3.4566459110064898,\n",
       "  3.490660990035394,\n",
       "  3.5004191609914415,\n",
       "  3.465085667994572,\n",
       "  3.543375704990467])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_experiment(datasets, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with r=0 redundant features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-45.6042452, -45.6042452, -45.6042452, -45.6042452, -45.6042452]]),\n",
       " [3.5810848900000565,\n",
       "  3.597158725024201,\n",
       "  3.631132123002317,\n",
       "  3.6066061799938325,\n",
       "  3.614835326996399])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_experiment(datasets, 2, '2step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "credit_experiment(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n",
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "credit_experiment(2, 'two-step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview TODOs\n",
    "- show classification unaffected by hallucincation\n",
    "  - formally\n",
    "  - experiments (just synth. data + income + other tab. dataset)\n",
    "- write this reconstruction impossibility in paper\n",
    "- e2e training vs. 2-step training\n",
    "  - synt. data + income + other tab. dataset\n",
    "- classification Income, Forest Cover Type, Higgs Dataset\n",
    "\n",
    "# Martin's Feedback\n",
    "- Change Title!\n",
    "- Abstract more specific (just Buzzwording til now)\n",
    "- same for Intro\n",
    "  - missing: more story, Inspiration TMLR paper\n",
    "  - ask: what is missing in FL? What does our solution provide?\n",
    "  - draw connection to PCs better\n",
    "  - streamline contribution list\n",
    "\n",
    "- Related work\n",
    "  - why are the works relevant we list there?\n",
    "  - can be a bit longer (maybe not necessary for WS paper)\n",
    "\n",
    "- \"A Probabilistic View on FL\"\n",
    "  - composability argument eher? decomposable? less probabilistic\n",
    "\n",
    "- Differenz hybrid/vertical/horizontal visualisieren\n",
    "  - Beispiel?\n",
    "  - Pooja's Intro figure?\n",
    "\n",
    "- 3.1.\n",
    "    - Assumptions bisschen weicher introducen\n",
    "    - connection zu Figure?\n",
    "    - Notation + Intuition sollte da sein\n",
    "\n",
    "- \"Hallucination\"\n",
    "  - Formalie bei Einführung SPN?\n",
    "  - Assunption Independence + Abmildern!\n",
    "\n",
    "- Def. PC etwas abändern, mind. citation aber eher umschreiben\n",
    "  - Fokus auf independence assumptions\n",
    "  - einfach mit PCs gehen, gar nicht hardcore definitions geben\n",
    "  - SPNs später\n",
    "\n",
    "- Hallucination\n",
    "  - falsche Annahme, die dazu führt\n",
    "  - Assumption broken\n",
    "\n",
    "- hybrid haben wir nicht, wir unifien\n",
    "  - mehr betonen bei Intro\n",
    "\n",
    "- Communication Costs auf jeden Fall für ICLR\n",
    "  - mit oder ohne Ring Reduce\n",
    "\n",
    "- Experiments (I)\n",
    "  - non-i.i.d. anders schreiben, wir machen ja non-iid\n",
    "  - \"although MNIST from global joint, client only partially observes dist.\"\n",
    "  - align with 3. marginalization\n",
    "\n",
    "- Experiments (II)\n",
    "  - binaranize MNIST generated images!!\n",
    "  - show likelihoods for Q1 (FedSPN vs. SPN)\n",
    "  - for ICLR: larger dataset\n",
    "  - WS: Only Q1 + Q4\n",
    "  - ICLR: Q2 generell raus\n",
    "  - larger scale tab. data (see TabNet) instead of images\n",
    "\n",
    "## Dev's feedback\n",
    "- related work hybrid FL\n",
    "- medical, credit etc.\n",
    "- SVHN\n",
    "- read through reviews and see what they have missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1]],\n",
       "\n",
       "        [[0, 1]],\n",
       "\n",
       "        [[0, 1]],\n",
       "\n",
       "        [[0, 1]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 1, 1, 1]).reshape(-1, 1)\n",
    "F.one_hot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- give name to indepence\n",
    "    - see marginals, joint is max-entropy dist. then we can take prod.\n",
    "    - argue: we want to maximize entropy and minimize uncertainty (over what?)\n",
    "    - say: many joints lead to same marginal, but we want to have independent marginals\n",
    "\n",
    "- However, ...\n",
    "    - instead say: if we make max. entropy ass. we get correct model and might be useful if ass. not met\n",
    "    - then introduce the following assumption\n",
    "\n",
    "- go for PCs instead of SPNs\n",
    "- Einet and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated-spn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
