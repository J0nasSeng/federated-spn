{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conditional_feinsum import init_spn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor\n",
    "from utils import set_einet_weights, extract_image_patches, get_surrounding_patches\n",
    "import numpy as np\n",
    "from nns import MLP\n",
    "import config\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "einet = init_spn(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einet_layers.0.ef_array.params\n",
      "torch.Size([64, 3, 1, 6])\n",
      "einet_layers.1.params\n",
      "torch.Size([3, 3, 3, 4])\n",
      "einet_layers.2.params\n",
      "torch.Size([3, 3, 1, 2])\n",
      "einet_layers.3.params\n",
      "torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = 0\n",
    "for n, p in einet.named_parameters():\n",
    "    print(n)\n",
    "    print(p.shape)\n",
    "    num_params += np.prod(list(p.shape))\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(), Resize(112), CenterCrop(112)])\n",
    "imagenet = ImageNet('/storage-01/datasets/imagenet/', transform=transform)\n",
    "clusters = np.load('/storage-01/ml-jseng/imagenet-clusters/vit_cluster_minibatch_10K.npy')\n",
    "img_ids = np.argwhere(clusters == 2400).flatten()\n",
    "subset = Subset(imagenet, img_ids)\n",
    "loader = DataLoader(subset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(einet, train_loader, num_epochs, i, j, device):\n",
    "\n",
    "    \"\"\"\n",
    "        Train ConFeinsum\n",
    "    \"\"\"\n",
    "    out_dims = []\n",
    "    for i, p in enumerate(einet.parameters()):\n",
    "        if i == 0:\n",
    "            nv, k, a, th = p.shape\n",
    "            # split mean and variance parameters as variance\n",
    "            # gets relu applied (cannot be negative)\n",
    "            out_dims.append([nv, k, a, th // 2])\n",
    "            out_dims.append([nv, k, a, th // 2])\n",
    "        else:\n",
    "            out_dims.append(list(p.shape))\n",
    "\n",
    "    in_dim = (3*config.num_dims*config.num_vars) + 1000\n",
    "    mlp = MLP(in_dim, out_dims, [256, 512]).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(mlp.parameters(), 0.01)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_ll = 0.0\n",
    "        for x, y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # patch images and obtain 8x8 patches\n",
    "            patches = extract_image_patches(x, config.height, config.width)\n",
    "            x_in = patches[:, :, i, j]\n",
    "            x_prev = get_surrounding_patches(patches, i, j, device)\n",
    "            # flatten all previous patches and concat along feature dimension for NN\n",
    "            x_prev = [x.squeeze().reshape(x_in.shape[0], -1) for x in x_prev]\n",
    "            x_prev = torch.cat(x_prev, dim=1)\n",
    "            y_oh = F.one_hot(y, num_classes=1000)\n",
    "            x_prev = torch.cat([x_prev, y_oh], dim=1)\n",
    "            einet_params = mlp(x_prev)\n",
    "            \n",
    "            ll = 0.\n",
    "            x_in = x_in.permute((0, 2, 3, 1))\n",
    "            x_in = x_in.reshape(x_in.shape[0], config.num_vars, config.num_dims)\n",
    "\n",
    "\n",
    "            # set einet parameters for each batch as parameters differ\n",
    "            # for each sample\n",
    "            for k in range(x.shape[0]):\n",
    "                einet_param = [ep[k] for ep in einet_params]\n",
    "                einet = set_einet_weights(einet, einet_param)\n",
    "\n",
    "                # evaluate log-likelihood\n",
    "                x_in_img = x_in[k].unsqueeze(0)\n",
    "\n",
    "                ll += einet(x_in_img).sum()\n",
    "\n",
    "            ll.backward()\n",
    "            total_ll += ll\n",
    "            #print(einet.einet_layers[0].ef_array.ll.grad.shape)\n",
    "            #print([(n, p.grad) for n, p in einet.named_parameters()])\n",
    "            #print([e.grad for e in einet_params])\n",
    "            print([(n, p.grad) for n, p in mlp.named_parameters()])\n",
    "            optim.step()\n",
    "        print(f\"Epoch {epoch} \\t LL: {total_ll}\")\n",
    "\n",
    "    return einet, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-jseng/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear_layers.0.weight', None), ('linear_layers.0.bias', None), ('linear_layers.1.weight', None), ('linear_layers.1.bias', None), ('heads.0.weight', None), ('heads.0.bias', None), ('heads.1.weight', None), ('heads.1.bias', None), ('heads.2.weight', None), ('heads.2.bias', None), ('heads.3.weight', None), ('heads.3.bias', None), ('heads.4.weight', None), ('heads.4.bias', None)]\n",
      "[('linear_layers.0.weight', None), ('linear_layers.0.bias', None), ('linear_layers.1.weight', None), ('linear_layers.1.bias', None), ('heads.0.weight', None), ('heads.0.bias', None), ('heads.1.weight', None), ('heads.1.bias', None), ('heads.2.weight', None), ('heads.2.bias', None), ('heads.3.weight', None), ('heads.3.bias', None), ('heads.4.weight', None), ('heads.4.bias', None)]\n",
      "[('linear_layers.0.weight', None), ('linear_layers.0.bias', None), ('linear_layers.1.weight', None), ('linear_layers.1.bias', None), ('heads.0.weight', None), ('heads.0.bias', None), ('heads.1.weight', None), ('heads.1.bias', None), ('heads.2.weight', None), ('heads.2.bias', None), ('heads.3.weight', None), ('heads.3.bias', None), ('heads.4.weight', None), ('heads.4.bias', None)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m einet \u001b[39m=\u001b[39m init_spn(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m einet, mlp \u001b[39m=\u001b[39m train(einet, loader, \u001b[39m5\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m, device)\n",
      "\u001b[1;32m/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     total_ll \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.83.185.147/home/ml-jseng/code/federated-spn/src/federated-einsum/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torchvision/datasets/folder.py:246\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "einet = init_spn(device)\n",
    "einet, mlp = train(einet, loader, 5, 2, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/AUxENR8TDhgcFg8eDiA0CQ4WEOzfBa2wqAIkPxPqAfIfJy/l6ewqJzUB9wzp7OcKCBECysTM8PbqAA/qP0E4+PfyGB4J4eTXT1VKAgsPFSoyNPUB+ePu1srN0wwEKSgcM8vAyQQHFgr9Cfby++/o2eQF/BUOBcI0NiUcIhMEJR0E3tvU9uoSKyQr0MPexwj5GSAI4OXaAv0D/ygrKO/q/9vT8B4YHvTf/ameyDAuQgJHQkAdHBHj2uIeFCEIBfv/Au01Kh+8qq6nllZOZzZtBQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/Ady8iNLf2CA7OltibzMsMujv33uDa+no7ALy7OYgDw+8nqsL/w/t7u5iXVO6trvy+AQCKzA+BwEQAgYAd49vJCwmr6vGDgom1MfeAvcUOeDo8xkUH863wzE0JRkmGNzY5QH4/gTItos7TT7n8P7ksq0EBgQdKRb8+wPl2d4CyMfW+wH26e/PWGRU/wAEu7LADwgIQkgvAtbk85KXsA4HJ83Q1dXP1JCEiunc4bu4uwT3+fYxJCiln6MOHRT8/fLg8c/q+OIyOTTIAmcQOPILZAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/AaLFqhMUGxsHBdre2aOZqw8SD9DZ6yURFgIrIyQSFyHD18oIDgMD/Qrk8foSCAjR1scC8+j5BNPS/fcB7dbm/QwM79vPAgD7LSg6AvUK/GB4eCskGOPc6M23sxAPECMZDeTp0wQK5uQaCg6tkqWdsctwgoG0wb3fv8Ty/v4BPDgn9fwU2Obpdn1l4NkAwraa7/nqBfkJBB4wLrqxwSUPBM/K1vvq7PHq8CBERhX+AATg290ZHhIKGDPs19gqPEs1+vXi8/D3+fE4I2Xl/wg6bAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/ATI1MzRLTe7cy+XP1O799DYtKwoO+gT8CwQXJiro4srs2+Pq+fgECv3Z2u30/OYNBRYC//bi/fcHdKiEfYWABvv/6vfoEwgZ+vzsAuDY4jBOOOXb3wEeDfDwAv/99tnj39/i8AIGBQlPTVMO+xIpFiLp8/Lx9P3///kNFv8CgaOIyLywRTtQAPn+DBAaExcd8vj/GgsjBP4GCQALHL3bwAL28AL96erlAiw0ORIIEgQI9ek8OUwO/vPk6NkgLjML/xD09vzM1cXVmmobo7m1UgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAqklEQVR4nAXB0VbCMAwA0CSNYYCbMmDwoH/hr/j/r+LxTBmjXRPaeC9+fPbo2A7v7f5twAegzxp/7r/c98cVy/ZwDptuHi9cyzTfohsRCvgaFUJKtKVKviSTp5bVynPXNdSMl6/SPtgw5cwsXMCTZkSc5hthYPN1s8ol8/40cNlo1pfdq0Glqg3XHK8EiMDBAYAIKlXzJcZpGnn8vookEVHVGv5MTEEXX/4BVJldhbFd7R4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAy0lEQVR4nAXBuW7CMBgAYB+/c+DEJqRGQFWGMiF16wQSL8EL9aW69wE6wgAjgUqgUq6GEh+N+T48fXvWxlp84bKmNjfaVtoBZcRufwYkHnWHvMrWxXVVnK4lSXifmLPTJz1+nTRQQrxpCZFL8W9vwJAYPL2Ux3oxL0CiiER52vZIg6n59qBn7x8Q5WFgwYm/A+ZJAL8Ofc6WN2ySR5Vx5p2KWaoyCobuakFxKF0Qd7iyImh1pbmU4D1mEEIjbT6odq/3XZ03q6/9fncH5rFV4azXMLwAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/AUNcFQoJDtvZ6OXg9iAbFOXk8gsKAvrz+gTt5PzvB/oSHw39DP3q2/P3/P4OFwQQEwwCDx8LHCYPJCQZBPQIHR4VDAQA/+8I9vj5BB8gChUPGREVEwUABsXA4Ck6Eufj+jY8JAQC/AbP0tj9+P/x7/YrMh7f2ef0/vYLFfoC7ev9OjpBFxUJIyUk6d/nLDIoJCMa+OsSAW92Yd3i3CQzC/Lr9QcLCvH24RkYH//+/gQLKvUI9/YDCQ0FEQX5/wIVGCzm7OsIBAdF7ldGONMegwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAyUlEQVR4nAXBvU7CUBQA4Ht+ei+CLLSkoRIHoo7GxIGVicTH8CV4MTYd3FxcTHQzxrhoCCam2FKh95zD98Hi9jobpuO8kLpdf63evj/OJuejwYiLNF+vfu4fXzS2l0W+KZvl3VMiLdxMs7LabSjM5levD8+nEP6S/ef7L0zGnUBoBA7RYkwDAfus98/bKu49HHVDXTVIIEqKTblFOBmyqKJBVFNmAvGJASI781FEXOywOaeJsSchRgwp9y6c6xsc97HLlcR6pyThAGvrVa+/0yHHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAo0lEQVR4nAXBSXLCMBAAwFkteUkZc+IzPIBX8HMunJJDqkgstFkjuvF+u+5IE488KUGGxutpDM9vKdwtl9+mQwqp18s0QeTZVKK50XWszUzQI2C3QCmR5ELZajHb/OpdPYeSjmLI8nrHzoBiVOMWcBncz9+rkRMkJZRWbQeQJo/9PX/Npasoq+KATKBg1v8zrkL1MFlmr2jLyA16CNEvGFPITT/vkVjEKw15LwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAo0lEQVR4nAXBMVIDMQwFUEn+8joEQmaSippTMHTc/xwUFAwLu7bXssR7/P72YTNkKblobVshYsKSHnF0Z4lha1slLyeIasl9HyAaEqEzGSkfRuxJZUQFI0PFhnFKAgLwu28UhvBm3dld03AzN76eSjDj9eXmPh6K9l6D5v3yTHas+8RN02ffKXC/PjGNtm2J89/R8fP9VaHWqgpB5vl8qc1SkX+n4VfoRV1SOgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAtUlEQVR4nAXBTVPCMBAA0DTdfJcWEK1TDjLDeOTkyb/kH3eAA5BpMe1mN75Xff8chh2U+4I5MzNmdI19dxJMbblA2zVO022cUmSx8JkSBM8v7UdTVkTjymMlLk+8V7UCC/1h+KKJ0zNaPSkMjY9i/oX1br/ffmYzprCZ/h5GWSW9FB0c+9MQ+qS7cY7rNtDsr7d6SQ/Yvh6NNixKIJeBscZNebtEAZ3zskgtFGldkABItxWT+AddZVVgCoodtwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAoUlEQVR4nAXB23HCMBAF0LsPS7IJhhk+kg6SGqiJltOBv2CMbGtXyzl0f/zORU9Tt2WJOF5BrXmPQTWOALqjBn1/Xa6w57pVCO/o3V0CGTz4FuZuaA7NIQRya+/V/qsXFQCRWY2lDWSMg3saZUrEpLdJWcQuiWYEMpT6nMefUb1urKmA9XyiW8rmUlvb7eiDcAk26c+ltuYleH3v+fwnef4AXTFWVZPewzsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAfUlEQVR4nAXBAU4DMRAEwZ7x2pwi5TV5KX9FBA57hyq9PjlbVmTtkzGJuBZ1Nh4gEKvs6iP31fV4al18f5EEcjbdmQPX0N8diQSFNFLuX9XPu41kTacPDpCQGkEJcHcUh6S1pq3pLM1HzXLS3fHIWKfYXaWPoe12edAaUPwD6ANHNPpE5GEAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAfElEQVR4nAXBSw5BURRE0V3n40W0tCSmYJbGKqGvhXdPWUu3OzKS9t1rUJJbLLmQFDKAPURiT0ERZPD7DQZFHEehgQpbgcBQSTd94nzKcjJjhAMdRkddL1y3qRKzw5BNNuvjx0vPNSUBELI8iwy+bzsoWtmw3CUvk8qwWn/tqDk92waZ5gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAkElEQVR4nAXBS1LDQAwFwPckeWycBbdgxf2PFYqkEjyjH938/jKjzoaWSwu7mgqBnTBaHoyB8VtzC6bUUtpl6+DeLes4j76Z3DqeewyDnundQvHUwtU/1WZyN40SjPA/d4+CYx/5LoOhPCNdyWqKbP1KoQL2mIEFOXtrDuGGfXG9s+zT5TGgkxC9ZtdAJKkf/3bOVTkcMIS9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAkElEQVR4nAXB2w0CMQwEQO/auRzi8UcFVED/RSEEAhLHNjO43y6Jjv6TOFe+BMnsgWFft2I0HRHLqNO3bkMd1vapYpnHrb0Sde6e0puJCbsEFIPzmvDJpVxFmMqj6uSlak9haBDJuX+sQ6YEc8gIsHwncuEt5nFUsB32dXJddK7IRhfzcrPS0AQ81yYKRon+AflbUo9ULKHZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAxUlEQVR4nAXBQWoCMRQA0CQ//yejE3VUFFsQtIuuxE0XBXHpBTxJz1l6hYILsSioNHFinJnM+B6fLNYG0BABY1H4wIuCAyZaZpNZFM0r5z/nKzKpoIx1vLlczN6mJLu7a7D2pt0/2ONAXFr+yPufW305bV6sDcU+3PWwkwwIVFt+rX4PMP87me5w/GFq787WlUiN/L4ve5kav6eaoH7kofQMkLQV2SjRaZsUoqKqQcDKGMZZR1KvT6xKEWIsWqrKmc/dgwifRtJMIvyALf4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAzklEQVR4nAXBO4rCQBgA4PnHf8YYxyjxUbnVgsew3wtssVfYxsYjeAy9gmCnaCUIgoWt2ywEGzG4JJmI88rs98F0Nnkfut0mqffqH2M1Xzw0D6POAG85vZzI91f/8MNW619odUNEDha1gUiI5bGSMhOxsFAQH/aaOaapC7zJnVMQEs6dtU1W3TOPn6P79swtUK5K/8oNeRogUnHcJzp+Y1FXUMBrYpw0bTANQjFoxdyX6u9mqWK1F9ScBIZBgLQAzYRADda4svJP6yuSZsU//mhgH653br4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAeklEQVR4nDXOsU0DARBE0fkze5bAohVix+6/BUROC3e76wA5f8Hj/vXZ3cDMJElSVY/nd3X37gJJZmZ3Jf3+/HF83GKMd3c04MpxbhdmpX/oGHL1WFu2tYuQtFoARG9Na2awgDkvmPhwVIAQACSeUVUFyraUlN83A7Zf+os5/uZ7QYgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAp0lEQVR4nAXBuU7DQBQF0LfcjNfEMVBQICSaWALR8P9fEyGBIB1F4vG8uZyjw/EpDXpaXr8+z/O8LsuSr4mAdtMgtbapzofx+fTy+/N3u2b6TqepR9OMex/67vtyiZwo9eFu1LbfORzwuoWYkmaGtqO5e78/FFpV30qJCJLbRgOactsah4qYgqSZRAmYac5ZhO5OEgDJ+8cj1jWLCFkjAkBKCcD7x9s/B5tL/6Iw+T4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=8x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x, y = subset[i]\n",
    "    x = x.unsqueeze(0)\n",
    "    patches = extract_image_patches(x, 8, 8)\n",
    "    x = patches[0, :, 2, 2]\n",
    "    s = get_surrounding_patches(patches, 2, 2)[0][0]\n",
    "    x = x * 255\n",
    "    x = x.permute(1, 2, 0)\n",
    "    s = s * 255\n",
    "    s = s.permute(1, 2, 0)\n",
    "    img = x.numpy().astype(np.uint8)\n",
    "    pimg = Image.fromarray(img)\n",
    "    pimg.show()\n",
    "    imgs = s.numpy().astype(np.uint8)\n",
    "    pimgs = Image.fromarray(imgs)\n",
    "    pimgs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 12, 3)\n",
    "b = torch.cat((a, a**2), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5499,  0.3395, -0.2652,  0.3024,  0.1153,  0.0703])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Variable(torch.randn(3, 3), requires_grad=True)\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.mm(x, w)\n",
    "x_ = torch.randn(3, 5)\n",
    "y_ = torch.mm(x_, y)\n",
    "l = torch.sum(y_)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1417, -0.1417, -0.1417],\n",
       "        [ 0.6123,  0.6123,  0.6123],\n",
       "        [ 0.3511,  0.3511,  0.3511]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceinsum import EinsumNetwork, Graph, EinetMixture\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor\n",
    "import numpy as np\n",
    "from nns import MLP\n",
    "import config\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "def init_spn(device):\n",
    "    \"\"\"\n",
    "        Build a SPN (implemented as an einsum network). The structure is either\n",
    "        the same as proposed in https://arxiv.org/pdf/1202.3732.pdf (referred to as\n",
    "        poon-domingos) or a binary tree.\n",
    "\n",
    "        In case of poon-domingos the image is split into smaller hypercubes (i.e. a set of\n",
    "        neighbored pixels) where each pixel is a random variable. These hypercubes are split further\n",
    "        until we operate on pixel-level. The spplitting is done randomly. For more information\n",
    "        refer to the link above.\n",
    "    \"\"\"\n",
    "\n",
    "    if config.structure == 'poon-domingos':\n",
    "        pd_delta = [[config.height / d, config.width / d] for d in config.pd_num_pieces]\n",
    "        graph = Graph.poon_domingos_structure(shape=(config.height, config.width), delta=pd_delta)\n",
    "    elif config.structure == 'binary-trees':\n",
    "        graph = Graph.random_binary_trees(num_var=config.num_vars, depth=config.depth, num_repetitions=config.num_repetitions)\n",
    "    elif config.structure == 'flat-binary-tree':\n",
    "        graph = Graph.binary_tree_spn(shape=(config.height, config.width))\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown Structure\")\n",
    "\n",
    "    args = EinsumNetwork.Args(\n",
    "            num_var=config.num_vars,\n",
    "            num_dims=config.num_dims,\n",
    "            num_classes=1,\n",
    "            num_sums=config.K,\n",
    "            num_input_distributions=config.K,\n",
    "            exponential_family=config.exponential_family,\n",
    "            exponential_family_args=config.exponential_family_args,\n",
    "            online_em_frequency=config.online_em_frequency,\n",
    "            online_em_stepsize=config.online_em_stepsize)\n",
    "    \n",
    "    in_dim = (3*config.num_dims*config.num_vars) + 1000\n",
    "    out_dims = [[config.num_vars, config.num_dims, 1, 3],\n",
    "                [config.num_vars, config.num_dims, 1, 3], \n",
    "                [config.num_dims, config.num_dims, config.num_dims, 4],\n",
    "                [config.num_dims, config.num_dims, 1, 2],\n",
    "                [1, 1, 2]]\n",
    "    mlp = MLP(in_dim, out_dims, [256, 512]).to(device)\n",
    "    einet = EinsumNetwork.EinsumNetwork(graph, mlp, args)\n",
    "    einet.initialize()\n",
    "    einet.to(device)\n",
    "    return einet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(), Resize(112), CenterCrop(112)])\n",
    "imagenet = ImageNet('/storage-01/datasets/imagenet/', transform=transform)\n",
    "clusters = np.load('/storage-01/ml-jseng/imagenet-clusters/vit_cluster_minibatch_10K.npy')\n",
    "img_ids = np.argwhere(clusters == 2400).flatten()\n",
    "subset = Subset(imagenet, img_ids)\n",
    "loader = DataLoader(subset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(einet, train_loader, num_epochs, i, j, device):\n",
    "\n",
    "    \"\"\"\n",
    "        Train ConFeinsum\n",
    "    \"\"\"\n",
    "\n",
    "    optim = torch.optim.Adam(einet.param_nn.parameters(), 0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_ll = 0.0\n",
    "        for x, y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            ll = einet(x, y, i, j)\n",
    "            ll.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad.clip_grad_norm_(einet.param_nn.parameters(), 1.)\n",
    "\n",
    "            total_ll += ll\n",
    "            #print(einet.einet_layers[0].ef_array.ll.grad.shape)\n",
    "            #print([(n, p.grad) for n, p in einet.named_parameters()])\n",
    "            #print([e.grad for e in einet_params])\n",
    "            #if torch.isnan(ll):\n",
    "            #    print(x)\n",
    "            #    print([(n, p) for n, p in einet.named_parameters()])\n",
    "            optim.step()\n",
    "        print(f\"Epoch {epoch} \\t LL: {total_ll}\")\n",
    "\n",
    "    return einet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-jseng/anaconda3/envs/federated-spn/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t LL: 889.8013916015625\n",
      "Epoch 1 \t LL: 875.5518798828125\n",
      "Epoch 2 \t LL: 853.3012084960938\n",
      "Epoch 3 \t LL: 816.2379150390625\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[ 1.9543,  1.1128,  1.6635, -1.9972, -1.4968, -2.1063]],\n",
      "\n",
      "         [[ 2.0179,  0.6993,  0.2842, -2.2048, -1.4901, -1.2029]],\n",
      "\n",
      "         [[ 0.1338,  0.6649,  1.1244, -1.2689, -1.4098, -1.4710]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9661,  1.6989,  0.4491, -2.8677, -2.0426, -1.3558]],\n",
      "\n",
      "         [[ 4.9348,  0.9646,  0.4408, -4.7240, -1.3430, -1.0660]],\n",
      "\n",
      "         [[ 0.4585,  0.1105,  1.0299, -1.1222, -1.1081, -1.6710]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5940,  3.4700,  4.0368, -1.1694, -3.3590, -3.9132]],\n",
      "\n",
      "         [[ 0.3891,  0.5651,  1.8896, -1.2498, -1.3941, -1.8666]],\n",
      "\n",
      "         [[ 0.8943,  0.3092,  0.3325, -1.3343, -1.0259, -1.2611]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0290,  0.1934,  0.4802, -1.5813, -1.0238, -1.0967]],\n",
      "\n",
      "         [[ 0.6587,  1.4598,  1.5264, -1.2259, -1.6965, -1.8763]],\n",
      "\n",
      "         [[ 1.8154,  1.9085,  0.5380, -2.2470, -1.9978, -1.1718]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3387,  0.5893,  0.5123, -1.8246, -1.2560, -1.2394]],\n",
      "\n",
      "         [[ 1.0171,  0.3842,  0.3515, -1.4846, -1.2600, -1.1408]],\n",
      "\n",
      "         [[ 0.2417,  2.0972,  0.9775, -1.1509, -2.1602, -1.6226]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3557,  0.9126,  2.8162, -1.1985, -1.4585, -3.2444]],\n",
      "\n",
      "         [[ 1.2537,  0.6318, -0.1950, -1.6013, -1.3055, -1.1255]],\n",
      "\n",
      "         [[ 0.6230,  2.5119,  3.9249, -1.1588, -3.0135, -3.8781]]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False)\n",
      "tensor([[[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Epoch 4 \t LL: nan\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "einet = init_spn(device)\n",
    "einet = train(einet, loader, 5, 2, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9915)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor(-2*-2e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated-spn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
